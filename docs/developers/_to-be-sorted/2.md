Here’s the implementation flow again, super informal—exactly how I’d talk it through to an AI agent updating the design.

# Stream → Workbook (how the engine actually runs)

Alright, here’s how it behaves in practice.

**0) Kickoff.**
We start the job, call `on_job_start(job)` (no return needed), set up logging/artifact, open the Excel file(s) in `read_only=True` mode so we can stream.

**1) First pass: stream rows to find tables.**
For each sheet:

* We iterate rows with streaming (values only).
* Run row detectors to label rows (header/data/separator/etc.).
* From those labels, we mark table regions: start row, end row, header row.
* We log all of that into the **artifact** (sheet name, region refs, which rules fired, scores).
  No heavy memory here—just a rolling window to detect boundaries.

**2) Second pass: per-table, load just enough for columns.**
Now that we know where tables live, we process them **one at a time**.

* For each detected region, we materialize that slice into memory.
* We create a real **OpenPyXL `Table`** object for that region (so we’ve got a concrete `ref`).
* For each column in that table, we get a proper **OpenPyXL column slice** (the “column object”), and we call the **column detectors with both**: `(job, table, column)`.
* Detectors return **scores only**. We pick the best mapping for each column, record the mapping + top rules into the artifact.
  So: **table + column in, scores out**. The engine owns the mapping decision.

**3) After-mapping hook (table-level cleanup).**
We call `after_mapping(job, table)` and pass the actual **OpenPyXL table**.
This hook can tweak headers, reorder columns, drop/add columns, small fixes—whatever. It returns the **same table** (possibly mutated). We proceed with that.

**4) Transform + validate (row-level).**
Still working on this table (in memory):

* We iterate rows (as dicts keyed by canonical field names).
* For each field, call `transform(field, value, row, job)`; it returns a dict of `{canonical_field: normalized_value}` (or `None`), and we merge that into the row. This supports one-to-many cases (e.g., `fullName` → `firstName` + `lastName`).
* Then we run validators for the row/fields; collect failures into the artifact.
  At the end, we’ve got a clean, canonicalized set of rows for this table.

**5) Write table back, move on.**
We write the normalized rows back into the sheet (or a normalized sheet for that table—implementation detail, but the result should be a coherent grid).
If a sheet had multiple detected tables, we **merge them** in sheet order into a single logical output per sheet (single header, stacked bodies). Then we repeat for the next table/next sheet.

**6) Build the final workbook.**
At this point we have a live **OpenPyXL `Workbook`** populated with all normalized content. Structure is stable, data is final, artifact has the full audit trail (detections, mappings, transforms, validations).

**7) Before save.**
Call `before_save(job, book)`. This is where cosmetic/project-level tweaks happen—rename tabs, add summary sheets, freeze panes, widths, whatever. It returns the **workbook**.

**8) Save + wrap up.**
We save the workbook to disk.
Call `on_job_end(job)` (no return). Done.

---

### Tiny pseudo-flow for the agent

```
on_job_start(job)

open workbook (read_only=True)
for each sheet:
  stream rows → run row detectors → record table regions in artifact

for each (sheet, region) in detected order:
  table = openpyxl.Table(ref=region)
  for each column in table:
    scores = run_column_detectors(job, table, column)
  mapping = choose_mapping(scores); artifact.record(mapping)

  table = after_mapping(job, table)  # may mutate, returns same table

  for each row in table.rows_as_dicts():
    for field in header:
      delta = transform(field, row[field], row, job)  # dict or None
      apply(delta)
    run_validators(row)  # record failures in artifact

  write_normalized_table_back(sheet, table)

merge_tables_per_sheet_if_needed()

book = build_final_workbook()
book = before_save(job, book)
save(book)

on_job_end(job)
```

Key principles baked in:

* Stream first, **detect structure cheaply**.
* Per-table work is **in-memory but bounded** (one table at a time).
* Column detectors get **both** the OpenPyXL table and the **column object**.
* Hooks are obvious: **after_mapping(table → table)**, **before_save(book → book)**; start/end return nothing.
* Everything important is mirrored into the **artifact** so runs are deterministic and auditable.
