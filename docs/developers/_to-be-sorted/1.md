## 0) What a config package is (and where things live)

A **config package** is a small folder you author that tells ADE *how to read* a messy spreadsheet. It contains:

```
my-config/
  manifest.json                         # Manifest v1.0 (required)
  requirements.txt                      # Optional (pinned) dependencies
  row_detectors/
    header.py                           # row detector functions (detect_*)
    data.py                             # row detector functions (detect_*)
  column_detectors/
    sin_number.py                       # column detector functions (detect_*), optional transform/validate
    member_id.py
    employee_number.py
  hooks/
    on_job_start.py                     # run()
    after_mapping.py                    # run()
    after_transform.py                  # run()
    after_validate.py                   # run()
    on_job_end.py                       # run()
  util/                                 # optional helpers you import from your detectors
    text.py
```

During **prepare**, ADE freezes this folder into a read‑only **snapshot** alongside a per‑version virtualenv (`venvs/<config_id>/activation/snapshot/`). Jobs import only from that snapshot, guaranteeing determinism.

---

## 1) The scoring model (the one mental model to hold)

ADE uses **additive scoring** in two places:

1. **Row scoring** (by **row_detectors**):
   For each row, your detectors add positive or negative **score deltas** to a small set of row labels (usually `header`, `data`, optionally `separator`). ADE **adds** those deltas from all row detectors and picks the label with the highest total. These labels define table boundaries and locate header rows.

2. **Column scoring** (by **column_detectors**):
   For each raw column in a detected table, your detectors add score deltas to one or **more** **target fields** (the normalized output fields declared in the manifest). ADE sums those deltas across all detectors and picks the field with the highest total—**if** it clears a gating threshold. Otherwise, the raw column remains **unmapped**.

> **Important—and new in this refined spec:**
> A **column detector may score several fields at once** (positive *and* negative). This is how you disambiguate lookalikes—e.g., a column that *looks like a SIN* will **raise** the score for `sin` and simultaneously **lower** the score for `member_id` and `employee_number`. Use one rule to share what it knows, instead of duplicating rules per field.

**Calibration rule of thumb**
Keep individual detector deltas roughly in **[-1.0, +1.0]**. Scores are additive and act like confidence signals. The manifest’s `mapping_score_threshold` guards against low‑confidence mappings.

---

## 2) Script API v1 — global rules for every function

* All public functions are **keyword‑only** (use `*`) and must accept **extra kwargs** via `**_` (forward compatible).
* Treat functions as **pure**: rely only on inputs provided; don’t read the clock, random, files, or the network.
* ADE runs your code in a **sandboxed subprocess** (network disabled by default; CPU/memory/file‑size caps).
* On exception, ADE records a **rule error** in the Artifact and continues the pass with a **neutral** result.

Common kwargs you’ll see:

```python
job_id: str
source_file: str             # filename only
sheet_name: str              # spreadsheet tab
table_id: str               # e.g., "table::sheet::1::1" (column/row passes only)
row_index: int              # 1-based (row passes only)
column_index: int           # 1-based within table (column passes only)
header: str | None          # normalized header text (column passes only)
row_values_sample: list     # small sample, this row (row passes only)
values_sample: list         # small sample, this column (column passes only)
values: list                # full column values (transform/validate)
field_name: str             # the manifest field for *this* file (column passes)
field_meta: dict            # manifest.columns.meta[field_name]
manifest: dict              # parsed manifest.json
env: dict | None            # manifest["env"] (or None)
artifact: dict              # read-only snapshot of the Artifact (append-only narrative)
```

---

## 3) Row detectors (Pass 1: find tables & headers)

**Goal in a sentence**: label each row as `header`, `data`, or `separator` by returning **score deltas** for those labels. ADE adds deltas from all row detectors and chooses the max; tables emerge from contiguous `data` blocks preceded by a `header`.

**Where**: `row_detectors/header.py` and `row_detectors/data.py`
**What you write**: one or more `detect_*` functions per file.

### Signature

```python
def detect_*( *,
    job_id: str,
    source_file: str,
    sheet_name: str,
    row_index: int,                       # 1-based
    row_values_sample: list,              # this row's values (lightweight)
    manifest: dict,
    env: dict | None,
    artifact: dict,
    **_
) -> dict:
    """
    Return:
      {"scores": {"header": float, "data": float, "separator": float}}
    Include only non-zero entries. Typical deltas in [-1.0, +1.0].
    """
```

### Examples

**Header by text density**

```python
# row_detectors/header.py
def detect_text_density(*, row_values_sample, **_):
    non_blank = [c for c in row_values_sample if c not in (None, "")]
    if not non_blank:
        return {"scores": {"separator": 0.2}}       # empty-ish rows separate blocks
    textish = sum(isinstance(c, str) for c in non_blank) / len(non_blank)
    return {"scores": {"header": 0.6}} if textish >= 0.7 else {"scores": {}}
```

**Data by number density**

```python
# row_detectors/data.py
def detect_numeric_density(*, row_values_sample, **_):
    digits = sum(str(c).strip().isdigit() for c in row_values_sample if c not in (None, ""))
    return {"scores": {"data": 0.6}} if digits >= 2 else {"scores": {}}
```

ADE adds these deltas row by row, labels rows, then infers tables and header positions.

---

## 4) Column detectors (Pass 2: map raw columns to fields)

**Goal in a sentence**: For each raw column, return score deltas for **one or more** manifest fields—positive for “looks like this field,” negative for “looks unlike this other field.” ADE totals deltas per field and picks the best above a threshold; otherwise, the column remains **unmapped**.

**Where**: `column_detectors/<field>.py`
**What you write**: one or more `detect_*` functions. In this refined model, any detector may **raise or lower scores for multiple fields**, not just the file’s `field_name`. (The file name is organizational; ADE considers all deltas for all fields.)

### Signature

```python
def detect_*( *,
    job_id: str,
    source_file: str,
    sheet_name: str,
    table_id: str,
    column_index: int,
    header: str | None,
    values_sample: list,
    field_name: str,                      # the field owning *this file*
    field_meta: dict,                     # manifest.columns.meta[field_name]
    manifest: dict,
    env: dict | None,
    artifact: dict,
    **_
) -> dict:
    """
    Return:
      {"scores": {
          "<field_id>": float,            # may include field_name and/or other field ids
          "<other_field_id>": float,
          ...
      }}
    Deltas should usually be in [-1.0, +1.0]. Unknown field ids are ignored.
    """
```

> ADE will **sum by field id** across all detectors in *all* `column_detectors/*.py` files. The final mapping for a raw column is the field with the highest total if it meets the gate (`mapping_score_threshold`). On a tie, the earlier field in `columns.order` wins.

### Examples (evolving power)

**Header synonyms (simple positive)**

```python
# column_detectors/member_id.py
def detect_synonyms(*, header, field_name, field_meta, **_):
    if not header: return {"scores": {}}
    h = header.lower()
    bump = sum(0.6 for syn in field_meta.get("synonyms", []) if syn in h)
    return {"scores": {field_name: min(1.0, bump)}}  # clamp to +1.0
```

**Value shape (regex) — positive and negative**

```python
# column_detectors/sin_number.py  (Canada SIN: 9 digits; fake example)
import re
SIN9 = re.compile(r"^\D*([0-9]\D*){9}$")

def detect_sin_shape(*, values_sample, **_):
    hits = sum(bool(SIN9.match(str(v))) for v in values_sample if v)
    ratio = hits / max(1, len(values_sample))
    # Positive for sin; slightly negative for lookalikes to reduce false positives
    return {"scores": {
        "sin": round(min(1.0, 0.7 * ratio), 2),
        "member_id": -0.3 if ratio > 0.6 else 0.0,
        "employee_number": -0.2 if ratio > 0.6 else 0.0
    }}
```

**Anti‑evidence (negative) from header contradictors**

```python
# column_detectors/member_id.py
def detect_not_member_headers(*, header, **_):
    if not header: return {"scores": {}}
    h = header.lower()
    if any(x in h for x in ["sin", "ssn", "social insurance"]):
        # A header that explicitly says SIN should push away from member_id
        return {"scores": {"member_id": -0.7, "sin": +0.2}}
    return {"scores": {}}
```

**Cross‑field ranking by domain list (positive for one, negative for others)**

```python
# column_detectors/employee_number.py
KNOWN_PREFIXES = {"emp", "employee", "staff"}

def detect_employee_prefix(*, header, **_):
    if not header: return {"scores": {}}
    h = header.lower()
    if any(p in h for p in KNOWN_PREFIXES):
        return {"scores": {"employee_number": +0.7, "member_id": -0.2, "sin": -0.4}}
    return {"scores": {}}
```

This pattern—**share the same evidence across multiple fields**—is how you dramatically reduce false positives without proliferating rules.

---

## 5) Transform (Pass 3: optional normalization)

**Goal in a sentence**: Given the **mapped** column’s values, return a normalized list (same length), plus optional warnings. Transforms do **not** write raw cell values into the Artifact; they return them to the engine which writes the normalized workbook.

**Where**: `column_detectors/<field>.py`

### Signature

```python
def transform( *,
    job_id: str,
    source_file: str,
    sheet_name: str,
    table_id: str,
    column_index: int,
    header: str | None,
    values: list,                         # full column values (row order)
    field_name: str,
    field_meta: dict,
    manifest: dict,
    env: dict | None,
    artifact: dict,
    **_
) -> dict:
    """
    Return:
      {"values": list, "warnings": list[str]}
    """
```

**Example (normalize IDs)**

```python
def transform(*, values, **_):
    def clean(v):
        if v is None: return None
        s = "".join(ch for ch in str(v) if ch.isalnum()).upper()
        return s or None
    out = [clean(v) for v in values]
    return {"values": out, "warnings": ["normalized to A-Z0-9; uppercased"] if out != values else []}
```

---

## 6) Validate (Pass 4: optional checks)

**Goal in a sentence**: Report **issues** (no data changes) with table‑relative coordinates. ADE writes these into the Artifact for dashboards/UI.

**Where**: `column_detectors/<field>.py`

### Signature

```python
def validate( *,
    job_id: str,
    source_file: str,
    sheet_name: str,
    table_id: str,
    column_index: int,
    header: str | None,
    values: list,                         # after transform
    field_name: str,
    field_meta: dict,
    manifest: dict,
    env: dict | None,
    artifact: dict,
    **_
) -> dict:
    """
    Return:
      {"issues": [
        {"row_index": int, "code": "str", "severity": "error"|"warning"|"info", "message": "str"}
      ]}
    """
```

**Example (required)**

```python
def validate(*, values, field_name, field_meta, **_):
    req = field_meta.get("required", False)
    if not req: return {"issues": []}
    issues = []
    for i, v in enumerate(values, start=1):
        if not v:
            issues.append({"row_index": i, "code": "required_missing", "severity": "error",
                           "message": f"{field_name} is required."})
    return {"issues": issues}
```

---

## 7) Hooks (optional, annotate only)

Hooks give you stable points to **annotate** the Artifact. They do not change core structures.

**Where**: `hooks/*.py`
**Order**: `on_job_start → after_mapping → after_transform → after_validate → on_job_end`

### Signature

```python
def run( *,
    artifact: dict,                       # read-only snapshot
    manifest: dict,
    env: dict | None,
    job_id: str,
    source_file: str,
    **_
) -> dict | None:
    """
    Return: {"notes": "short message"} | None
    """
```

**Example**

```python
def run(*, artifact, **_):
    total = 0
    for s in artifact.get("sheets", []):
        for t in s.get("tables", []):
            total += len(t.get("validation", {}).get("issues", []))
    return {"notes": f"Total issues: {total}"}
```

---

## 8) Manifest v1.0 (declarative control plane)

`manifest.json` declares the **engine defaults**, **writer behavior**, **hooks**, and the **columns** (target fields). It also references your detector files.

### Minimal manifest (copy‑paste)

```json
{
  "config_script_api_version": "1",
  "info": { "schema": "ade.manifest/v1.0", "title": "Membership Rules", "version": "1.0.0" },
  "engine": {
    "defaults": {
      "timeout_ms": 120000,
      "memory_mb": 256,
      "runtime_network_access": false,
      "mapping_score_threshold": 0.25
    },
    "writer": { "append_unmapped_columns": true, "unmapped_prefix": "raw_" }
  },
  "env": { "LOCALE": "en-CA" },
  "hooks": {
    "on_job_start":   [{ "script": "hooks/on_job_start.py" }],
    "after_mapping":  [{ "script": "hooks/after_mapping.py" }],
    "after_transform":[{ "script": "hooks/after_transform.py" }],
    "after_validate": [{ "script": "hooks/after_validate.py" }],
    "on_job_end":     [{ "script": "hooks/on_job_end.py" }]
  },
  "columns": {
    "order": ["sin", "member_id", "employee_number"],
    "meta": {
      "sin": {
        "label": "SIN",
        "required": true,
        "script": "column_detectors/sin_number.py",
        "synonyms": ["sin", "ssn", "social insurance"],
        "type_hint": "string"
      },
      "member_id": {
        "label": "Member ID",
        "required": false,
        "script": "column_detectors/member_id.py",
        "synonyms": ["member id", "member no", "member#"],
        "type_hint": "string"
      },
      "employee_number": {
        "label": "Employee #",
        "required": false,
        "script": "column_detectors/employee_number.py",
        "synonyms": ["employee", "emp id", "staff no"],
        "type_hint": "string"
      }
    }
  }
}
```

### Field semantics (succinct)

* `config_script_api_version`: must be `"1"` (this spec).
* `info.schema`: must be `"ade.manifest/v1.0"`.
* `engine.defaults`:

  * `timeout_ms`, `memory_mb`: per‑rule budgets; enforced by runtime.
  * `runtime_network_access`: default **false**; keep off unless you must call network inside rules.
  * `mapping_score_threshold`: gate for auto‑mapping a raw column to a field (e.g., `0.25`).
* `engine.writer`:

  * `append_unmapped_columns`: whether to append unmapped raw columns to output.
  * `unmapped_prefix`: prefix for appended headers (e.g., `raw_`).
* `env`: free‑form strings available to all rules (e.g., `"LOCALE":"en-CA"`).
* `hooks`: arrays of `{"script":"hooks/<file>.py"}`; each must export `run()`.
* `columns.order`: output order and tie‑breaker when scores tie.
* `columns.meta[<field>].script`: path to your **column_detectors** file for that field.
  *(Row detectors are auto‑discovered in `row_detectors/`.)*

ADE validates this manifest during **prepare**. If versions or required fields are wrong, prepare fails early.

---

## 9) How ADE aggregates your scores (column mapping pseudocode)

```python
totals = {field_id: 0.0 for field_id in manifest["columns"]["meta"].keys()}
contributors = {field_id: [] for field_id in totals}

for file in all("column_detectors/*.py"):
    for rule in functions_named("detect_*", file):
        deltas = safe_call(rule, context)  # {"scores": {...}}
        for field_id, delta in deltas.get("scores", {}).items():
            if field_id not in totals:     # ignore scores for unknown fields
                continue
            totals[field_id] += delta
            contributors[field_id].append({"rule_id": rule.id, "delta": round(delta, 3)})

best_field, best = argmax(totals)
if best < manifest["engine"]["defaults"]["mapping_score_threshold"]:
    target_field = None  # unmapped
else:
    target_field = break_ties_by_columns_order(best_field, totals, manifest)
```

ADE then records in the Artifact: the winning `target_field`, its final `score`, and the top contributor rules (by absolute delta) for explainability.

---

## 10) Negative scoring (pattern to reduce false positives)

* **Do** penalize near‑misses: if a column *almost* looks like `sin`, reduce `member_id` and `employee_number` a bit.
* **Do** apply small negatives for contradictory headers (`"SSN"`, `"SIN"`) when you’re scoring other IDs.
* **Don’t** over‑penalize: a big negative can drown out real positives from other detectors. Stay in the **[-1.0, +1.0]** band per rule.

---

## 11) Techniques that scale well

* **Compose detectors**: tiny, single‑purpose `detect_*` rules (header synonyms, value shape, token ratios, domain lists) compose additively.
* **Share evidence**: a single detector can influence multiple fields (positive **and** negative). You get better precision with fewer rules.
* **Use `env`** for locale differences without branching on file paths.
* **Keep transforms cheap**: heavy normalization belongs here; validators can then be simple.

---

## 12) Quick smoke tests (copy into your repo)

**Confirm a detector’s shape**

```python
def test_sin_shape_scores_multiple_fields():
    from column_detectors.sin_number import detect_sin_shape
    ret = detect_sin_shape(values_sample=["123456789","A","999 999 999"])
    assert "scores" in ret and "sin" in ret["scores"]
    # Optional: it may also reduce lookalikes
    assert all(k in ret["scores"] for k in ("member_id","employee_number"))
```

**Confirm transform preserves length**

```python
def test_transform_len():
    from column_detectors.member_id import transform
    values = [" ab-12 ", None, "x"]
    out = transform(values=values)
    assert len(out["values"]) == len(values)
```

**Validate manifest**

```python
import json, jsonschema, pathlib
def test_manifest_v1_schema():
    schema = json.loads(pathlib.Path("docs/schemas/ade.manifest.v1.0.schema.json").read_text())
    manifest = json.loads(pathlib.Path("my-config/manifest.json").read_text())
    jsonschema.validate(manifest, schema)
```

---

## 13) Naming recap (consistency everywhere)

* Folder names: **`row_detectors/`** and **`column_detectors/`**.
* Functions: **`detect_*`** for detectors, **`transform`**, **`validate`**, and hook **`run`**.
* Manifest paths point to **`column_detectors/<field>.py`** for each field.
* ADE auto‑discovers **`row_detectors/header.py`** and **`row_detectors/data.py`**.

---

## 14) Final checklists

**When writing detectors**

* [ ] Only return `{"scores": {...}}` with field ids (column) or row labels (row).
* [ ] Keep deltas in [-1.0, +1.0]; use multiple small hints, not one giant rule.
* [ ] Consider **negative deltas** for near‑miss fields to reduce false positives.
* [ ] Accept `**_`; don’t crash on extra kwargs.

**When writing a manifest**

* [ ] `"config_script_api_version": "1"` and `"info.schema": "ade.manifest/v1.0"`.
* [ ] Every field in `columns.order` exists in `columns.meta`.
* [ ] Each field’s `script` points to `column_detectors/<field>.py`.
* [ ] `mapping_score_threshold` reflects your appetite for auto‑mapping (e.g., 0.25–0.5).

---

## 15) The contract you can build on

* **Script API v1** (this doc) is the executable boundary.
* **Manifest v1.0** (this doc) is the declarative boundary.

Lock these in. Your detectors can be creative, but their inputs and outputs must match these contracts. ADE’s pipeline and Artifact depend on it—and because your **column detectors can now score multiple fields (positive and negative)**, you can express rich disambiguation logic with fewer, cleaner rules.


Awesome—let’s keep building the mental model and put real weight behind it. This is a **hands‑on guide** that deepens the two boundaries you now own:

* **Config Package Script API v1** (row_detectors, column_detectors, transform, validate, hooks)
* **Config Package Manifest v1.0** (defaults, writer, env, hooks, columns)

Everything below uses your **canonical names** (`row_detectors/`, `column_detectors/`) and leans into the **scoring model**—including **cross‑field positive & negative scoring** to cut false positives.

---

## 1) One model to remember: additive scoring

Every detector you write returns **score deltas**. ADE **adds** those deltas across all detectors to make decisions. Two places:

1. **Row scoring** (by `row_detectors/*`): your detectors vote `{"header": +…, "data": +…, "separator": +…}` on each row. ADE adds up votes and labels the row. Tables come from runs of `data` rows bounded by a `header` (or separated by `separator`).
2. **Column scoring** (by `column_detectors/*`): for each raw column in a table, your detectors vote `{"<field>": +… | -…, "<other_field>": +… | -…}`. ADE totals by field and picks the winner **if** the best score ≥ `mapping_score_threshold` (from the manifest). Otherwise the raw column is **unmapped**.

**Calibration:** keep each detector’s deltas in **[-1.0, +1.0]**. Let multiple small hints blend together rather than writing a single extreme rule.

---

## 2) Row detectors — from rows to tables

> **Purpose:** label rows so ADE can infer tables and header rows.
> **Where:** `row_detectors/header.py`, `row_detectors/data.py`
> **What you author:** one or more `detect_*` functions per file.

### Signature (Script API v1)

```python
def detect_*( *,
    job_id: str,
    source_file: str,
    sheet_name: str,
    row_index: int,                 # 1-based
    row_values_sample: list,        # the row's values (lightweight)
    manifest: dict,
    env: dict | None,
    artifact: dict,                 # read-only snapshot
    **_
) -> dict:
    """
    Return:
      {"scores": {"header": float, "data": float, "separator": float}}
    Include only non-zero entries. Rule-of-thumb deltas in [-1.0, +1.0].
    """
```

### Introduce evidence slowly: two tiny rules

**Header by text density**

```python
# row_detectors/header.py
def detect_text_density(*, row_values_sample, **_):
    non_blank = [c for c in row_values_sample if c not in (None, "")]
    if not non_blank:
        return {"scores": {"separator": 0.2}}   # empty-ish row: weak separator hint
    textish = sum(isinstance(c, str) for c in non_blank) / len(non_blank)
    return {"scores": {"header": 0.6}} if textish >= 0.7 else {"scores": {}}
```

**Data by numeric density**

```python
# row_detectors/data.py
def detect_numeric_density(*, row_values_sample, **_):
    digits = sum(str(c).strip().isdigit() for c in row_values_sample if c not in (None, ""))
    return {"scores": {"data": 0.6}} if digits >= 2 else {"scores": {}}
```

ADE adds these, labels each row, then groups `data` blocks preceded by a `header` into tables (with coordinates). If no header is found, ADE may synthesize a header so column mapping can still proceed.

---

## 3) Column detectors — mapping raw columns to fields (with cross‑field scoring)

> **Purpose:** decide which **target field** (from the manifest) a raw column best represents.
> **Where:** `column_detectors/<field>.py` (organizational); ADE aggregates across **all** files.
> **What you author:** one or more `detect_*` functions.

### Signature (Script API v1)

```python
def detect_*( *,
    job_id: str,
    source_file: str,
    sheet_name: str,
    table_id: str,
    column_index: int,              # 1-based within table
    header: str | None,             # normalized header
    values_sample: list,            # small representative sample
    field_name: str,                # the field owning THIS file
    field_meta: dict,               # manifest.columns.meta[field_name]
    manifest: dict,
    env: dict | None,
    artifact: dict,                 # read-only snapshot
    **_
) -> dict:
    """
    Return:
      {"scores": {
         "<field_id>": float,       # can include field_name and/or other fields
         "<other_field>": float
      }}
    """
```

> **Key idea:** A **single detector** can **raise** the score for one field while **lowering** scores for confusingly similar fields. You reuse the same evidence twice—once as a **positive** and again as **anti‑evidence**—instead of writing two separate rules.

### Practical ladder for deltas

| Evidence strength                                   | Suggested delta |
| --------------------------------------------------- | --------------: |
| Exact synonym in header                             |    +0.6 to +0.8 |
| Strong value pattern match                          |    +0.4 to +0.7 |
| Weak hint (token overlap)                           |    +0.1 to +0.3 |
| Contradictory header (“SSN” when scoring member_id) |    −0.4 to −0.7 |
| Near‑miss pattern (looks like SIN but not quite)    |    −0.2 to −0.4 |

### Step‑by‑step examples

**(a) Positive: header synonyms**

```python
# column_detectors/member_id.py
def detect_synonyms(*, header, field_name, field_meta, **_):
    if not header: return {"scores": {}}
    h = header.lower()
    score = sum(0.6 for syn in field_meta.get("synonyms", []) if syn in h)
    return {"scores": {field_name: min(1.0, score)}}
```

**(b) Positive & negative: SIN (Canada) with simple shape**

```python
# column_detectors/sin_number.py
import re
SIN9 = re.compile(r"^\D*([0-9]\D*){9}$")   # “9 digits somewhere in the cell”

def detect_sin_shape(*, values_sample, **_):
    hits = sum(bool(SIN9.match(str(v))) for v in values_sample if v)
    ratio = hits / max(1, len(values_sample))
    # Share this evidence across fields: up SIN, down others
    return {"scores": {
        "sin": round(min(1.0, 0.7 * ratio), 2),
        "member_id": -0.3 if ratio > 0.6 else 0.0,
        "employee_number": -0.2 if ratio > 0.6 else 0.0
    }}
```

**(c) Anti‑evidence from header tokens**

```python
# column_detectors/member_id.py
def detect_conflicting_headers(*, header, **_):
    if not header: return {"scores": {}}
    h = header.lower()
    if any(x in h for x in ["sin", "ssn", "social insurance"]):
        return {"scores": {"member_id": -0.7, "sin": +0.2}}
    return {"scores": {}}
```

**(d) Domain prefix** *(positive for one, negative for others)*

```python
# column_detectors/employee_number.py
KNOWN_PREFIXES = {"emp", "employee", "staff"}

def detect_employee_prefix(*, header, **_):
    if not header: return {"scores": {}}
    h = header.lower()
    if any(p in h for p in KNOWN_PREFIXES):
        return {"scores": {"employee_number": +0.7, "member_id": -0.2, "sin": -0.4}}
    return {"scores": {}}
```

### How ADE decides

1. Sum deltas per field across all detectors for this raw column.
2. Let `best_field = argmax(totals)`.
3. If `totals[best_field] < mapping_score_threshold` → **unmapped** (safer than guessing).
4. On ties, earlier field in `columns.order` wins.

ADE writes the decision (score + top contributing rules) into the **Artifact** for explainability.

---

## 4) Transform — normalize values (optional)

> **Purpose:** return a cleaned list of values (same length) for the **mapped** field, plus optional warnings.
> **Where:** `column_detectors/<field>.py` (optional)

### Signature

```python
def transform( *,
    job_id: str,
    source_file: str,
    sheet_name: str,
    table_id: str,
    column_index: int,
    header: str | None,
    values: list,                   # full column values (row order)
    field_name: str,
    field_meta: dict,
    manifest: dict,
    env: dict | None,
    artifact: dict,
    **_
) -> dict:
    """
    Return: {"values": list, "warnings": list[str]}
    """
```

**Example (uppercased alphanumeric IDs)**

```python
def transform(*, values, **_):
    def clean(v):
        if v is None: return None
        s = "".join(ch for ch in str(v) if ch.isalnum()).upper()
        return s or None
    out = [clean(v) for v in values]
    warn = ["normalized to A-Z0-9 upper"] if out != values else []
    return {"values": out, "warnings": warn}
```

> Keep heavy normalization here, not in validators; validators will then be simpler and faster.

---

## 5) Validate — report issues (optional)

> **Purpose:** emit structured issues (no data changes).
> **Where:** `column_detectors/<field>.py` (optional)

### Signature

```python
def validate( *,
    job_id: str,
    source_file: str,
    sheet_name: str,
    table_id: str,
    column_index: int,
    header: str | None,
    values: list,                   # after transform
    field_name: str,
    field_meta: dict,
    manifest: dict,
    env: dict | None,
    artifact: dict,
    **_
) -> dict:
    """
    Return:
      {"issues": [
        {"row_index": int, "code": "str", "severity": "error"|"warning"|"info", "message": "str"}
      ]}
    """
```

**Example (required)**

```python
def validate(*, values, field_name, field_meta, **_):
    if not field_meta.get("required", False): return {"issues": []}
    issues = []
    for i, v in enumerate(values, start=1):
        if not v:
            issues.append({"row_index": i, "code": "required_missing", "severity": "error",
                           "message": f"{field_name} is required."})
    return {"issues": issues}
```

---

## 6) Hooks — annotate the Artifact (optional)

> **Purpose:** add **notes** at stable points. Hooks **annotate**; they **do not** mutate core structures.
> **Where:** `hooks/*.py`
> **Order:** `on_job_start → after_mapping → after_transform → after_validate → on_job_end`

### Signature

```python
def run( *,
    artifact: dict,                 # read-only
    manifest: dict,
    env: dict | None,
    job_id: str,
    source_file: str,
    **_
) -> dict | None:
    """
    Return: {"notes": "short message"} | None
    """
```

**Example**

```python
def run(*, artifact, **_):
    tables = sum(len(s.get("tables", [])) for s in artifact.get("sheets", []))
    return {"notes": f"Processed {tables} table(s)"}
```

---

## 7) Manifest v1.0 — the declarative control plane

Your `manifest.json` declares:

* **Engine defaults** (timeouts, memory, network policy, mapping threshold)
* **Writer behavior** (append unmapped)
* **Env** (free‑form strings for your rules)
* **Hooks** (files and when to run them)
* **Columns** (the target fields, their labels/order and which detector file owns them)

### Minimal usable manifest

```json
{
  "config_script_api_version": "1",
  "info": { "schema": "ade.manifest/v1.0", "title": "Membership Rules", "version": "1.0.0" },
  "engine": {
    "defaults": {
      "timeout_ms": 120000,
      "memory_mb": 256,
      "runtime_network_access": false,
      "mapping_score_threshold": 0.25
    },
    "writer": { "append_unmapped_columns": true, "unmapped_prefix": "raw_" }
  },
  "env": { "LOCALE": "en-CA" },
  "hooks": {
    "on_job_start":   [{ "script": "hooks/on_job_start.py" }],
    "after_mapping":  [{ "script": "hooks/after_mapping.py" }],
    "after_transform":[{ "script": "hooks/after_transform.py" }],
    "after_validate": [{ "script": "hooks/after_validate.py" }],
    "on_job_end":     [{ "script": "hooks/on_job_end.py" }]
  },
  "columns": {
    "order": ["sin", "member_id", "employee_number"],
    "meta": {
      "sin": {
        "label": "SIN",
        "required": true,
        "script": "column_detectors/sin_number.py",
        "synonyms": ["sin","ssn","social insurance"],
        "type_hint": "string"
      },
      "member_id": {
        "label": "Member ID",
        "required": false,
        "script": "column_detectors/member_id.py",
        "synonyms": ["member id","member no","member#"],
        "type_hint": "string"
      },
      "employee_number": {
        "label": "Employee #",
        "required": false,
        "script": "column_detectors/employee_number.py",
        "synonyms": ["employee","emp id","staff no"],
        "type_hint": "string"
      }
    }
  }
}
```

**Semantics to remember**

* `mapping_score_threshold` is the **confidence gate**: below it, ADE won’t auto‑map.
* `columns.order` is both the output column order **and** the tie‑breaker.
* `script` points to a **column_detectors** file.
* `synonyms` are just hints—your detectors decide how much to add for a match.

---

## 8) Craftsmanship: recipes that make detectors great

### 8.1 One detector, many fields

Use the same evidence positively and negatively:

```python
def detect_token_overlap(*, header, **_):
    if not header: return {"scores": {}}
    h = header.lower()
    scores = {}
    if "sin" in h or "social insurance" in h:
        scores["sin"] = scores.get("sin", 0) + 0.7
        scores["member_id"] = scores.get("member_id", 0) - 0.3
        scores["employee_number"] = scores.get("employee_number", 0) - 0.3
    return {"scores": scores}
```

### 8.2 Combine header + values (synergy)

Two moderate positives can beat a single strong positive:

```python
def detect_email_shape(*, values_sample, **_):
    hits = sum("@" in str(v) for v in values_sample if v)
    ratio = hits / max(1, len(values_sample))
    return {"scores": {"email": round(min(0.6, ratio), 2)}}
```

### 8.3 Guardrails for noise

Prefer small negatives for weak contradictions, bigger negatives only for strong contradictions. No single rule should swamp everything.

---

## 9) Debugging & introspection (Artifact‑first)

After a run, the **Artifact** explains every mapping decision. A helper snippet you can paste into a notebook:

```python
def explain_mapping(artifact, table_id, raw_col_idx):
    for s in artifact["sheets"]:
        for t in s["tables"]:
            if t["id"] != table_id: continue
            for m in t.get("mapping", []):
                if m["raw"]["column_index"] == raw_col_idx:
                    return {
                      "target_field": m["target_field"],
                      "score": m["score"],
                      "contributors": sorted(m.get("contributors", []),
                                             key=lambda c: abs(c["delta"]), reverse=True)
                    }
```

This will show the **top contributing detectors** and their deltas—perfect for tuning thresholds and weights.

---

## 10) Performance & determinism tips

* **Pre‑compile regex** at module import (as we did with `SIN9 = re.compile(...)`).
* **Use `values_sample`** for detectors; reserve full `values` for transform/validate.
* **No randomness, no time‑based logic** in detectors; keep them pure and predictable.
* **Cache static lists** (synonyms, token sets) in module globals.
* **Short‑circuit early** on `header is None` or empty samples; return `{"scores": {}}`.

---

## 11) A tiny starter package you can copy

```
my-config/
  manifest.json
  row_detectors/
    header.py
    data.py
  column_detectors/
    sin_number.py
    member_id.py
    employee_number.py
  hooks/
    after_validate.py
```

**row_detectors/header.py**

```python
def detect_text_density(*, row_values_sample, **_):
    non_blank = [c for c in row_values_sample if c not in (None, "")]
    if not non_blank: return {"scores": {"separator": 0.2}}
    ratio = sum(isinstance(c, str) for c in non_blank) / len(non_blank)
    return {"scores": {"header": 0.6}} if ratio >= 0.7 else {"scores": {}}
```

**column_detectors/sin_number.py** (as above)

**hooks/after_validate.py**

```python
def run(*, artifact, **_):
    err = sum(len(t.get("validation", {}).get("issues", []))
              for s in artifact.get("sheets", []) for t in s.get("tables", []))
    return {"notes": f"{err} issue(s) found"}
```

---

## 12) DOs & DON’Ts (field‑tested)

**DO**

* Write several small `detect_*` rules; let them add up.
* Use **negative deltas** for near‑miss fields to deflate false positives.
* Keep deltas small; let multiple pieces of evidence win together.
* Tune `mapping_score_threshold` with real data (start 0.25–0.5).

**DON’T**

* Put raw values into the Artifact (privacy & size).
* Depend on global machine state (time, randomness, disk, network).
* Over‑penalize: one −1.0 can erase three good positives; keep negatives measured.

---

## 13) Quick contract tests (drop‑in)

```python
def test_detector_multi_field_scoring():
    from column_detectors.sin_number import detect_sin_shape
    ret = detect_sin_shape(values_sample=["123 456 789", "hello", None])
    assert "scores" in ret and any(k in ret["scores"] for k in ("sin","member_id","employee_number"))

def test_transform_len():
    from column_detectors.member_id import transform
    values = [" ab-12 ", None, "x"]
    out = transform(values=values)
    assert len(out["values"]) == len(values)
```

---

### Wrap‑up

* You now have a **single vocabulary** (row_detectors, column_detectors), a **single scoring model** (additive deltas), and a **single way** to express disambiguation (cross‑field positive & negative scores).
* Your **manifest** defines the target fields and the gate; your **detectors** provide evidence; the **Artifact** tells the story after each pass.

This is the ground you can stand on to author powerful, precise rules—**with fewer detectors, less duplication, and far fewer false positives.**


Below is a **from‑the‑ground‑up guide** to how ADE’s **virtual environments** work—what they are, why they exist, what is written to disk, how they’re built, when they’re rebuilt, and exactly how jobs use them. We’ll introduce each concept slowly and keep the vocabulary you now own (row_detectors, column_detectors, Manifest v1.0, Script API v1, Artifact).

---

## 1) Why virtual environments exist (one promise to remember)

ADE runs **untrusted, user‑authored rules**—your row_detectors and column_detectors—from **config packages**. To keep runs **deterministic** and **safe**, ADE never imports those scripts from your live project. Instead, for each **published config version**, ADE builds a **sealed runtime**:

> **Prepare once, run many.**
> Freeze the config’s code and dependencies **once** into a private Python virtual environment; reuse it for every job.

This gives us three guarantees:

* **Isolation** — rules import only from their snapshot; the global site‑packages are invisible.
* **Determinism** — the same config + the same input yields the same output (and Artifact), regardless of future edits.
* **Performance** — jobs don’t reinstall dependencies; they spawn into a ready interpreter.

---

## 2) What the “Activation Environment” contains

For each published config (think: **one immutable version**), ADE creates a folder under `${ADE_DATA_DIR}/venvs/<config_id>/` with two key pieces:

1. A dedicated **Python interpreter** (PEP 405 `venv/`) at `venvs/<config_id>/bin/python`.
2. An **activation** subfolder with everything needed to explain and replay the environment:

```
${ADE_DATA_DIR}/venvs/<config_id>/
  ├─ bin/python                         # the interpreter used for jobs of this config version
  └─ activation/
       ├─ snapshot/                     # frozen copy of the config package (import root at runtime)
       ├─ packages.txt                  # exact installed package versions (output of `pip freeze`)
       ├─ build.json                    # metadata (hashes, python version, prepared_at)
       └─ install.log                   # stdout/stderr from `pip install` (diagnostics)
```

**The snapshot** is the import root for your Script API v1 modules:

* `row_detectors/*.py`
* `column_detectors/*.py`
* `hooks/*.py`
* `util/` (optional helpers)
* `manifest.json` (also embedded into the job’s context for your functions)

At runtime, ADE points `PYTHONPATH` to `activation/snapshot/`, launches the **venv** interpreter with strict flags, and your detectors run inside that sealed world.

---

## 3) How ADE builds an activation environment (“prepare”)

The **prepare** step happens when you publish or activate a config version. It is **idempotent** and **content-aware**. You can think of it as a small pipeline:

1. **Hash the inputs (what determines identity)**

   * `content_hash` — hash of the package’s **code & manifest** (everything in the config folder except `requirements.txt`).
   * `deps_hash` — hash of **requirements** (`requirements.txt` content; empty if missing).
   * `py_version` — major.minor of the base Python ADE is using to create the venv.

2. **Skip or build**

   * If `venvs/<config_id>/activation/build.json` exists and all three (`content_hash`, `deps_hash`, `py_version`) match, **prepare is a no‑op**.
   * Otherwise, ADE builds into a **temporary folder** and swaps it into place atomically at the end.

3. **Create the venv**

   * Use the host’s Python to create `venvs/<config_id>/`.
   * Ensure global site‑packages are **not** visible from this venv.

4. **Install dependencies (if any)**

   * If `requirements.txt` exists, run `pip install -r requirements.txt` under the venv.
   * Use a persistent pip cache at `${ADE_DATA_DIR}/cache/pip/`.
   * If you set `ADE_WHEELHOUSE=/some/path`, ADE adds `--find-links` to prefer local wheels (fast, reproducible).
   * Capture stdout/stderr into `activation/install.log`.

5. **Freeze**

   * Run `pip freeze` and write **exact versions** to `activation/packages.txt`.

6. **Snapshot the package**

   * Copy the entire config package folder into `activation/snapshot/`.
   * This copy is **read‑only** for jobs (ADE runs with `-I -B` and will not write `.pyc`).

7. **Write metadata**

   * Write `activation/build.json` with:

     ```json
     {
       "content_hash": "…",
       "deps_hash": "…",
       "prepared_at": "2025-11-04T16:00:00Z",
       "python_version": "3.11.7",
       "pip_version": "24.2"
     }
     ```

8. **Swap directory atomically**

   * Move the temporary build into `venvs/<config_id>/` in one rename so any concurrent readers see either the old build or the new one—never a half‑written tree.

> **Safety note**
> Prepare does not execute your config code. It only copies files and installs dependencies. Rules run later, inside sandboxed worker processes, with network **off by default**.

---

## 4) When ADE rebuilds (and when it does not)

ADE rebuilds the activation environment only if one of these **rebuild triggers** changes:

* **Package content** — any change to your detectors, hooks, or manifest → new `content_hash`.
* **Dependencies** — any change to `requirements.txt` file content → new `deps_hash`.
* **Python version** — the venv’s base interpreter major.minor changed.

If none of the above changed, prepare returns immediately (idempotent). Jobs keep using the existing environment.

> **Tip for stability**
> Prefer **pinned versions** in `requirements.txt` when you care about byte‑for‑byte reproducibility across time and machines. The `packages.txt` we capture ensures replay of what was installed, but pinning avoids surprises at build time.

---

## 5) How jobs use the activation environment (the job runtime story)

When a job starts, ADE creates a small working directory, e.g. `${ADE_DATA_DIR}/jobs/<job_id>/`. Inside it:

```
jobs/<job_id>/
  inputs/                      # copies or links to your uploaded spreadsheet(s)
  artifact.json               # append-only narrative (see Artifact v1.1)
  normalized.xlsx             # final output
  events.ndjson               # enqueue/start/progress/finish
  run-request.json            # snapshot of parameters given to the worker
  .venv → ../../venvs/<config_id>/      # symlink to the prepared environment
```

The worker then:

* Reads `run-request.json` (policy, manifest, rule registry).
* Spawns **`.venv/bin/python -I -B`**, which:

  * **-I** runs in isolated mode (ignores user site).
  * **-B** prevents writing `.pyc` bytecode (no mutation).
* Sets `PYTHONPATH` to `activation/snapshot/`, so `import column_detectors.member_id` loads the **frozen** script you prepared.
* Runs the five passes (**Find → Map → Transform → Validate → Generate**) and saves `artifact.json` after each pass.

> **Result:** Your Script API v1 functions run in a clean, sealed world with the exact dependencies you declared at prepare time. They cannot see global site‑packages or touch the network (unless the job’s policy enables it).

---

## 6) Offline and air‑gapped operation (wheelhouse)

In secure environments or CI without internet access you can **pre‑stage wheels** (prebuilt packages) and point ADE to them:

* Set `ADE_WHEELHOUSE=/opt/ade/wheels` (or any path containing `.whl` files).
* ADE adds `--find-links` to `pip install` so every dependency is satisfied from local wheels.
* Optionally add `--no-index` (hard offline) if all dependencies are covered by the wheelhouse.

Because we still capture `packages.txt`, jobs remain **explainable** and **replayable**.

---

## 7) Determinism and safety (mechanics, not guidelines)

* **Imports:** jobs import only from `activation/snapshot/` (plus stdlib and venv site‑packages).
* **Isolation:** worker subprocess sets **CPU**, **memory**, and **file size** limits; networking is blocked by default.
* **No bytecode:** `-B` avoids `.pyc` creation; snapshots stay bit‑identical across runs.
* **Atomic writes:** prepare atomically writes the activation folder; jobs atomically write `artifact.json`.
* **No ambient site‑packages:** the worker’s `-I` and the venv’s isolation prevent leakage from the host Python.

---

## 8) Developer ergonomics (what you see & how to debug)

When a prepare fails, look at:

* `venvs/<config_id>/activation/install.log` — why `pip install` failed.
* `venvs/<config_id>/activation/packages.txt` — what actually got installed.
* `venvs/<config_id>/activation/build.json` — hashes and timestamps; confirm whether a rebuild was skipped.

If a job fails at runtime, the **Artifact** has `rule_errors[]` entries; the **events.ndjson** shows lifecycle events.

---

## 9) Show, don’t tell — the prepare function (reference implementation)

Below is an **implementation sketch** (simplified but faithful). You can drop this into `apps/ade-api/src/ade_api/kernel/build/prepare.py` and wire it to your “activate config” path.

```python
# apps/ade-api/src/ade_api/kernel/build/prepare.py
from __future__ import annotations
import json, os, shutil, subprocess, sys, tempfile, time, hashlib
from dataclasses import dataclass
from pathlib import Path

from ..core.paths import venvs_dir, pip_cache_dir
from ..core.time import now_iso

@dataclass
class BuildMeta:
    content_hash: str
    deps_hash: str
    prepared_at: str
    python_version: str
    pip_version: str

def _hash_dir(root: Path, ignore: set[str]) -> str:
    h = hashlib.sha256()
    for p in sorted(root.rglob("*")):
        rel = str(p.relative_to(root)).replace("\\", "/")
        if any(rel == pat or rel.startswith(pat + "/") for pat in ignore): continue
        h.update(rel.encode("utf-8"))
        if p.is_file():
            h.update(p.read_bytes())
    return h.hexdigest()

def _hash_requirements(cfg_root: Path) -> str:
    req = cfg_root / "requirements.txt"
    return hashlib.sha256(req.read_bytes()).hexdigest() if req.exists() else "no-reqs"

def _read_build_json(path: Path) -> dict | None:
    try: return json.loads(path.read_text(encoding="utf-8"))
    except FileNotFoundError: return None

def prepare_config(config_id: str, cfg_root: Path, *, wheelhouse: Path | None = None) -> Path:
    """
    Build or reuse the activation environment for a published config.
    Returns the absolute path to activation/snapshot/.
    """
    cfg_root = cfg_root.resolve()
    vroot = venvs_dir() / config_id
    activation = vroot / "activation"
    snapshot = activation / "snapshot"
    build_json = activation / "build.json"
    packages_txt = activation / "packages.txt"
    install_log = activation / "install.log"

    # 1) Compute identity
    content_hash = _hash_dir(cfg_root, ignore={"requirements.txt"})
    deps_hash = _hash_requirements(cfg_root)
    py_version = f"{sys.version_info.major}.{sys.version_info.minor}"

    # 2) Idempotency: skip if unchanged
    current = _read_build_json(build_json)
    if current and current.get("content_hash") == content_hash and \
       current.get("deps_hash") == deps_hash and \
       current.get("python_version") == py_version and snapshot.exists():
        return snapshot

    # 3) Build into a temp dir and swap atomically
    tmp_root = Path(tempfile.mkdtemp(prefix=f"ade-build-{config_id}-"))
    try:
        # 3a) Create venv
        subprocess.run([sys.executable, "-m", "venv", str(tmp_root)], check=True)

        # venv python and pip
        py = tmp_root / ("Scripts/python.exe" if os.name == "nt" else "bin/python")
        pip = [str(py), "-m", "pip"]

        # 3b) Upgrade pip (isolated)
        subprocess.run(pip + ["install", "--disable-pip-version-check", "--no-input", "pip>=23.2"], check=True)

        # 3c) Install dependencies (if any)
        req = cfg_root / "requirements.txt"
        pip_args = [*pip, "install", "--disable-pip-version-check", "--no-input",
                    "--cache-dir", str(pip_cache_dir())]
        if wheelhouse:
            pip_args += ["--find-links", str(wheelhouse)]
        if req.exists():
            with install_log.open("w", encoding="utf-8") as log:
                proc = subprocess.run(pip_args + ["-r", str(req)], stdout=log, stderr=log)
                if proc.returncode != 0:
                    raise RuntimeError(f"pip install failed; see {install_log}")

        # 3d) Freeze exact versions
        packages = subprocess.check_output(pip + ["freeze"], text=True)
        packages_txt.parent.mkdir(parents=True, exist_ok=True)
        packages_txt.write_text(packages, encoding="utf-8")

        # 3e) Snapshot package
        snap_dir = snapshot
        snap_dir.parent.mkdir(parents=True, exist_ok=True)
        shutil.copytree(cfg_root, snap_dir, dirs_exist_ok=True)

        # 3f) Write build.json
        pip_version = subprocess.check_output(pip + ["--version"], text=True).strip().split()[1]
        meta = BuildMeta(
            content_hash=content_hash,
            deps_hash=deps_hash,
            prepared_at=now_iso(),
            python_version=py_version,
            pip_version=pip_version,
        )
        build_json.write_text(json.dumps(meta.__dict__, indent=2), encoding="utf-8")

        # 3g) Move temp venv into place atomically
        #   tmp_root is the venv; we need the parent structure
        target = vroot
        if target.exists():
            shutil.rmtree(target)  # safe because jobs reference .venv symlink per-job
        shutil.move(str(tmp_root), str(target))
        return snapshot
    finally:
        # If we errored before swap, clean the temp
        if tmp_root.exists() and not (venvs_dir()/config_id).exists():
            shutil.rmtree(tmp_root, ignore_errors=True)
```

> In production you’ll likely add a simple **file lock** (e.g., `venvs/<config_id>.lock`) so two prepare requests don’t race. The atomic swap already ensures consistency; the lock avoids wasted duplicate work.

---

## 10) Job launcher (how the venv is consumed)

For completeness, here’s the **worker spawn** sketch that consumes the prepared environment:

```python
# apps/ade-api/src/ade_api/features/jobs/launcher.py (sketch)
import json, os, subprocess
from pathlib import Path

def launch_worker(job_dir: Path, config_id: str, policy: dict, run_request: dict) -> subprocess.Popen:
    vroot = Path(os.environ["ADE_DATA_DIR"]).resolve() / "venvs" / config_id
    python = vroot / ("Scripts/python.exe" if os.name == "nt" else "bin/python")
    snapshot = vroot / "activation" / "snapshot"

    env = os.environ.copy()
    env["PYTHONPATH"] = str(snapshot)
    env["ADE_RUNTIME_NETWORK_ACCESS"] = "true" if policy.get("runtime_network_access") else "false"

    # -I isolated, -B no .pyc
    proc = subprocess.Popen(
        [str(python), "-I", "-B", "-m", "ade_worker.entrypoint"],
        cwd=str(job_dir),
        env=env,
        stdin=subprocess.PIPE,
        stdout=subprocess.PIPE,
        stderr=subprocess.PIPE,
        text=True,
    )
    proc.stdin.write(json.dumps(run_request))
    proc.stdin.flush()
    return proc
```

---

## 11) Acceptance checklist (what “done” looks like)

* Publishing a config **creates or reuses** `${ADE_DATA_DIR}/venvs/<config_id>/`.
* `activation/snapshot/` contains an exact copy of your config package.
* `activation/packages.txt` lists exact installed versions.
* `activation/build.json` records `content_hash`, `deps_hash`, `prepared_at`, `python_version`.
* **Idempotency**: running prepare again with identical inputs does **nothing** (no new writes).
* **Rebuild**: changing any script or `requirements.txt` creates a new activation folder content and updates `build.json`.
* A job creates `.venv → ../../venvs/<config_id>/` and uses `bin/python -I -B` with `PYTHONPATH=activation/snapshot/`.

---

## 12) FAQ (things you’ll naturally ask)

**Q: Why not store the environment in the database?**
A: We want fast, atomic, **file‑based** startup and easy **diff/backup** semantics. Wheels can be large; file systems handle them gracefully. SQLite stores pointers and checksums only.

**Q: Do jobs get the latest build if I republish?**
A: Each published config has a unique **config_id**. Jobs reference a specific config_id; republishing produces a **new** config_id and a **new** activation directory. Old jobs remain reproducible because their snapshot never changes.

**Q: What about compiled wheels / native libs?**
A: If you depend on native extensions, pre‑build them for your target platform(s) and use `ADE_WHEELHOUSE`. This avoids non‑deterministic builds from source.

**Q: Can my rules download models at prepare time?**
A: Not in Script API v1. Keep prepare **non‑executing** for user code. If you need assets, bake them into the package or the wheelhouse, or download them in a **controlled build step** before you run ADE prepare.

---

### The story so far (tying it together)

* Your **Manifest v1.0** declares fields, thresholds, and writer behavior.
* Your **Script API v1** rules (row_detectors, column_detectors, transform, validate) express **evidence** via **additive scores**—including **negative scores** for lookalikes.
* ADE’s **Activation Environment** freezes that logic and its dependencies once, then every job reuses it—fast, safe, and reproducible.
* The **Artifact** is saved after every pass, giving you a crystal‑clear narrative of what happened and why.

With these three pillars—**Manifest**, **Script API**, and **Activation Environment**—the rest of ADE is plumbing. And because we built them to be **deterministic by construction**, your users can trust the output and your team can evolve the engine without breaking packages.


Perfect—let’s keep building in the same clear, cumulative style. You already own three pillars:

1. **Config Package Manifest v1.0** — the declarative contract.
2. **Config Package Script API v1** — row_detectors, column_detectors, transform, validate, hooks.
3. **Activation Environment (prepare once, run many)** — the sealed runtime per config.

Now we teach the **next essential concept** that ties those together in motion:

> **The Streaming Pass Pipeline** — how ADE reads a messy spreadsheet, decides where tables are, maps raw columns to target fields, cleans & checks values, and writes the normalized workbook — all while updating the Artifact after each pass.

We’ll introduce each piece slowly, in the order the engine uses them, and we’ll reuse your vocabulary (row_detectors, column_detectors, scoring, Artifact, activation snapshot). By the end, you’ll be able to imagine the whole job run as one small, deterministic loop.

---

## 1) The mental picture: five passes, one backbone

A job is a single, linear journey through **five passes**:

1. **Structure** (Pass 1): Label rows (header/data/separator) → infer tables + header row.
2. **Mapping** (Pass 2): For each table, use column_detectors to map raw columns → manifest fields via **additive scoring** (including negative deltas to avoid false positives).
3. **Transform** (Pass 3): For each **mapped** field/column, normalize values (same length list) and record per‑column warnings.
4. **Validate** (Pass 4): For each **mapped** field/column, emit issues (no data changes).
5. **Generate** (Pass 5): Write the normalized workbook in manifest field order, optionally appending **unmapped** columns as `raw_<header>`.

After each pass, ADE writes the **Artifact** atomically (append‑only), so the job is auditable, resumable, and deterministic.

---

## 2) How ADE reads spreadsheets (bounded and deterministic)

Before any pass runs, the worker is inside the **activation environment** (your snapshot is the only import root). ADE opens the source file through a **SpreadsheetReader** with two simple guarantees:

* **Streaming rows**: sheets are read row‑by‑row; ADE doesn’t load an entire workbook into memory.
* **Deterministic sampling**: when detectors need a column sample (`values_sample`), ADE uses a **stable policy** (e.g., first K non‑blank values plus a fixed stride), so your scoring is reproducible.

A very small, internal protocol keeps this simple:

```python
class SpreadsheetReader:
    def iter_sheets(self) -> Iterable[SheetCursor]: ...
class SheetCursor:
    name: str
    def iter_rows(self) -> Iterable[Row]: ...         # yields Row(index, values)
    def sample_column(self, a1_range: str, column_index: int, k: int) -> list: ...
    def column_values(self, a1_range: str, column_index: int) -> list: ...
```

You don’t implement this; ADE does. Your detectors receive **samples** or **full lists** via the Script API v1 kwargs.

---

## 3) Pass 1 — Structure (row_detectors & table inference)

We begin by answering one simple question, row by row: **what kind of row is this?** Your functions in `row_detectors/header.py` and `row_detectors/data.py` return **score deltas** for labels like `"header"`, `"data"`, `"separator"`. ADE adds deltas across all detectors, picks the **max**, and labels the row. With those labels, it infers **tables**:

* A **table** starts at a `header` row and continues through one or more `data` rows.
* `separator` rows (truly blank or low-signal) split tables.
* If convincing data appears before any header, ADE may synthesize a header so column mapping can proceed.

**Structure pass (sketch)**

```python
for sheet in reader.iter_sheets():
    labels = []
    for row in sheet.iter_rows():
        deltas = aggregate(detect(row) for detect in row_detectors)  # {"header": +…, "data": +…}
        labels.append(argmax(deltas, order=["header","data","separator"]))
    tables = infer_tables_from(labels)  # ranges + header row index
    artifact.sheets.append(serialize_tables(sheet.name, tables))
save_artifact(...)
```

**Why this comes first:** column_detectors need table ranges and header rows to map raw columns correctly.

---

## 4) Pass 2 — Mapping (column_detectors & additive scoring)

Now that we know table boundaries and header rows, we map each **raw column** in a table to a **target field** from your manifest. This is where your **column_detectors** shine — and where **cross‑field positive & negative scoring** eliminates false positives.

### The scoring rule (one paragraph to remember)

Every `detect_*` in `column_detectors/*.py` returns `{"scores": {"<field>": delta, ...}}`. ADE **adds** those deltas by field across **all** detectors and chooses the field with the **highest total**, provided it clears `mapping_score_threshold`. If no field clears the gate, the raw column remains **unmapped**. On ties, earlier fields in `columns.order` win. ADE records the final `score` and the **top contributing rules** in the Artifact.

**A single detector can score several fields** — positively and negatively — at once:

```python
# column_detectors/sin_number.py
def detect_sin_shape(*, values_sample, **_):
    hits  = sum(is_sin(v) for v in values_sample if v)
    ratio = hits / max(1, len(values_sample))
    return {"scores": {
        "sin":             min(1.0, 0.7 * ratio),     # positive for SIN
        "member_id":      -0.3 if ratio > 0.6 else 0, # nudge away near-miss fields
        "employee_number":-0.2 if ratio > 0.6 else 0
    }}
```

This is the **heart** of disambiguation. With it, you reuse evidence instead of duplicating rules.

**Mapping pass (sketch)**

```python
for table in artifact.sheets[i]["tables"]:
    for raw_col in table.columns():
        totals = {field: 0.0 for field in manifest.columns.meta.keys()}
        contributors = {f: [] for f in totals}
        for det in column_detectors:
            for field, delta in det(..., values_sample=sample):
                if field not in totals: continue
                totals[field] += delta
                contributors[field].append({"rule_id": det.id, "delta": round(delta, 3)})
        best = argmax(totals)
        if totals[best] < mapping_score_threshold:
            target_field = None
        else:
            target_field = break_ties_with_columns_order(best, manifest)
        table.mapping.append(serialize_mapping(raw_col, target_field, totals, contributors))
save_artifact(...)
```

---

## 5) Pass 3 — Transform (column‑wise normalization, then stream)

With mappings fixed, ADE prepares to **write rows** to the normalized workbook. Before writing, it gives each **mapped field** the full list of raw values for its column and lets your `transform` return a **clean list** of the same length, plus warnings.

* Keep transforms **pure** and **idempotent**: same input list → same output list.
* A transform can do heavy lifting once (e.g., strip punctuation, fix casing, parse dates) so validators stay simple.

```python
# column_detectors/member_id.py
def transform(*, values, **_):
    def clean(v): return "".join(ch for ch in str(v) if ch.isalnum()).upper() or None if v else None
    out = [clean(v) for v in values]
    warn = ["normalized to A-Z0-9 upper"] if out != values else []
    return {"values": out, "warnings": warn}
```

ADE stores only **warnings** in the Artifact (not the transformed values) to keep the Artifact small and private.

---

## 6) Pass 4 — Validate (issues, not changes)

Validators receive the **post‑transform** list and return **issues** with table‑relative coordinates. ADE adds A1 coordinates for display and aggregates totals by severity.

```python
# column_detectors/member_id.py
def validate(*, values, field_name, field_meta, **_):
    if not field_meta.get("required"): return {"issues": []}
    issues = [{"row_index": i, "code": "required_missing", "severity": "error",
               "message": f"{field_name} is required."}
              for i, v in enumerate(values, 1) if not v]
    return {"issues": issues}
```

The Artifact’s `validation` block ends up with per‑table totals and a list of issues (capped for size safety).

---

## 7) Pass 5 — Generate (row‑streaming writer & unmapped columns)

Finally, ADE writes **`normalized.xlsx`**. The writer is **row‑streaming**: it never accumulates the entire sheet in memory.

* Column order follows `manifest.columns.order`.
* If `append_unmapped_columns: true`, ADE appends any unmapped raw columns to the right as `raw_<header>`.
* **One header row** is written per table; ADE streams rows beneath it.

**Writer interface (simplified)**

```python
with NormalizedWorkbookWriter(job_dir/"normalized.xlsx") as w:
    for table in tables:
        headers = [manifest.columns.meta[f]["label"] for f in manifest.columns.order]
        if writer.append_unmapped_columns:
            headers += [f"raw_{c.header_normalized}" for c in table.unmapped]
        w.start_table(sheet_name=table.name, headers=headers)
        for r in range(table.row_count):
            row = []
            for f in manifest.columns.order:
                row.append(transformed_values[f][r])
            if writer.append_unmapped_columns:
                for c in table.unmapped:
                    row.append(table.raw_values[c.index][r])
            w.write_row(row)
save_artifact(...); close_workbook()
```

> The **Artifact** records that `normalized.xlsx` path under `output.normalized_workbook`, plus timing and counts under `summary`.

---

## 8) Putting it all together — the orchestrator loop

This is the “show me” that turns concepts into one small, predictable loop. Everything you see here already respects your Script API v1, Manifest v1.0, scoring model, activation environment, and artifact discipline.

```python
def run_job(reader, writer, manifest, rule_registry, job_dir):
    # 0) Artifact already initialized; rules attached
    art = load_artifact(job_dir/"artifact.json")

    # 1) Structure
    t0=time.time()
    art.sheets = pass_structure(reader, row_detectors)          # tables with ranges + header rows
    art.mark_pass(1, "structure", t0, time.time()); save(art)

    # 2) Mapping
    t0=time.time()
    for sheet in art.sheets:
        for table in sheet["tables"]:
            pass_mapping(reader, table, column_detectors, manifest, art)  # additive scoring (+ and −)
    art.mark_pass(2, "mapping", t0, time.time()); save(art)

    # 3) Transform (column-wise)
    t0=time.time()
    transformed = pass_transform(reader, art, manifest, column_detectors)
    art.mark_pass(3, "transform", t0, time.time()); save(art)

    # 4) Validate
    t0=time.time()
    pass_validate(art, transformed, manifest, column_detectors)
    art.mark_pass(4, "validate", t0, time.time()); save(art)

    # 5) Generate (row-streaming)
    t0=time.time()
    pass_generate(writer, art, transformed, manifest)           # writes normalized.xlsx
    art.output["normalized_workbook"] = "normalized.xlsx"
    art.summary = compute_summary(art)
    art.mark_pass(5, "generate", t0, time.time()); save(art)

    return art
```

**Resumability comes for free**: if the process stops after Pass 2, ADE reloads the Artifact, sees which passes are complete (via `pass_history`), and continues with 3→5.

---

## 9) Practical tuning and invariants

* **Scoring band**: keep individual detector deltas in **[-1.0, +1.0]**. Let evidence blend.
* **Confidence gate**: start `mapping_score_threshold` around **0.25–0.5**; tune with real data.
* **Unmapped is OK**: prefer unmapped columns over wrong mappings. You can append unmapped columns.
* **Privacy**: the Artifact records **decisions, not raw values**. Only the workbook holds data.
* **Atomicity**: the Artifact is written atomically after each pass; the workbook is written to a temp file and atomically moved into place.

---

## 10) Where this fits with what you know

* Your **manifest** defined “what we want” (fields, thresholds, writer rules).
* Your **detectors** expressed “why a column looks like a field” via **additive scoring**, including **negative deltas** to disambiguate.
* The **activation environment** made the runtime sealed and reproducible.
* The **pipeline** you just learned is the rhythm that keeps everything bounded, explainable, and deterministic — and it’s the same rhythm every job follows.

With the **Streaming Pass Pipeline** clear, you can now reason confidently about performance, correctness, and where to place code: **detectors for evidence**, **transforms for cleanup**, **validators for rules**, and **the writer for production of the normalized workbook** — all narrated by the Artifact.

If you’d like, the natural next lesson is **“Job lifecycle and back‑pressure”**: how we accept work with **202/429**, run with a **bounded queue and worker subprocesses**, and write **events.ndjson** so the UI can stream progress.
